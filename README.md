# ğŸš¢ Shipping ETL Pipeline  
Pipeline de descarga, limpieza y transformaciÃ³n de datos con Python

Este proyecto implementa un pipeline ETL completo utilizando Python.  
Incluye:

- Descarga automÃ¡tica de datos desde una URL
- Limpieza y transformaciÃ³n del dataset
- Logging profesional (archivo + consola)
- OrquestaciÃ³n del pipeline mediante `main.py`
- Estructura modular y reproducible
- Uso de entorno virtual, `.env` y `.gitignore`

Es un ejemplo claro y profesional de un flujo de trabajo tÃ­pico en Data Engineering.

---

## ğŸ“ Estructura del proyecto

î·™î·š
shipping_etl_project/ â”‚ â”œâ”€â”€ data/ â”‚   â”œâ”€â”€ raw/                # Datos originales descargados â”‚   â””â”€â”€ clean/              # Datos procesados por el ETL â”‚ â”œâ”€â”€ logs/ â”‚   â”œâ”€â”€ etl.log             # Logs del proceso ETL â”‚   â””â”€â”€ pipeline.log        # Logs del orquestador â”‚ â”œâ”€â”€ src/ â”‚   â”œâ”€â”€ download_data.py    # Script de descarga â”‚   â”œâ”€â”€ etl.py              # Script ETL con transformaciones â”‚   â””â”€â”€ main.py             # Orquestador del pipeline â”‚ â”œâ”€â”€ .env                    # Variables de entorno (rutas y URL) â”œâ”€â”€ .gitignore              # Exclusiones para Git â”œâ”€â”€ requirements.txt        # Dependencias del proyecto â””â”€â”€ README.md               # DocumentaciÃ³n del proyecto

---

## âš™ï¸ InstalaciÃ³n y configuraciÃ³n

### 1. Clonar el repositorio

```bash
git clone <URL_DEL_REPO>
cd shipping_etl_project


2. Crear entorno virtual
python -m venv venv
source venv/Scripts/activate   # Windows (Git Bash)


3. Instalar dependencias
pip install -r requirements.txt


4. Configurar variables de entorno
Archivo .env:
RAW_DATA_PATH=../data/raw/shipping_data.csv
CLEAN_DATA_PATH=../data/clean/shipping_data_clean.csv
DATA_URL=https://raw.githubusercontent.com/plotly/datasets/master/2014_usa_states.csv

î·™î·š

ğŸš€ EjecuciÃ³n del pipeline completo
Desde la carpeta src/:
python main.py


Esto ejecuta:
- download_data.py â†’ descarga el dataset
- etl.py â†’ limpia, transforma y guarda los datos
- Logging en logs/pipeline.log y logs/etl.log

ğŸ§¹ Transformaciones realizadas en el ETL
El script etl.py realiza:
- NormalizaciÃ³n de nombres de columnas
- Limpieza de espacios en strings
- ConversiÃ³n de columnas numÃ©ricas
- EliminaciÃ³n de duplicados
- EliminaciÃ³n de filas con valores crÃ­ticos nulos
- CreaciÃ³n de columna derivada:
population_per_rank = population / rank
- OrdenaciÃ³n por poblaciÃ³n
- Logging detallado de cada paso

ğŸ“ Ejemplo de salida
Archivo generado:
data/clean/shipping_data_clean.csv


Incluye columnas limpias, tipos corregidos y mÃ©tricas derivadas.

ğŸ§  Objetivo del proyecto
Este proyecto demuestra:
- Buenas prÃ¡cticas de Data Engineering
- Modularidad y separaciÃ³n de responsabilidades
- Uso de variables de entorno
- Logging profesional
- OrquestaciÃ³n de pipelines
- Reproducibilidad y claridad para recruiters
Es ideal como portfolio para roles de:
- Data Analyst
- Data Engineer
- Data Scientist (nivel junior/intermedio)

ğŸ“Œ PrÃ³ximas mejoras (roadmap)
- AÃ±adir validaciÃ³n de esquema (pydantic o pandera)
- AÃ±adir tests unitarios (pytest)
- AÃ±adir visualizaciones automÃ¡ticas
- Dockerizar el pipeline
- Integrar un scheduler (Airflow, Prefect o cron)

ğŸ‘¤ Autor
Juan M. de la Fuente Larrocca
Data Analyst & Data Engineer
Madrid
