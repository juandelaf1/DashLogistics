warning: in the working copy of 'src/etl/scrapers/fuel_scraper.py', CRLF will be replaced by LF the next time Git touches it
[1mdiff --git a/main.py b/main.py[m
[1mindex 4e074e4..876a297 100644[m
[1m--- a/main.py[m
[1m+++ b/main.py[m
[36m@@ -7,14 +7,14 @@[m [mimport os[m
 from src.utils.download_data import download_dataset[m
 from src.etl.etl import run_etl[m
 from src.etl.scrapers.fuel_scraper import scrape_fuel_prices[m
[31m-from src.etl.scrapers.update_master_data import update_everything[m
[32m+[m[32m# from src.etl.scrapers.update_master_data import update_everything[m
 from src.etl.enrichment.weather_api import get_weather_data[m
 # Generar un run_id para trazabilidad y a√±adirlo como filtro de logging[m
 import uuid[m
 [m
 # Asegurarnos de que cada ejecuci√≥n tenga un ID √∫nico (se puede pasar desde entorno)[m
[31m-RUN_ID = os.getenv('PIPELINE_RUN_ID') or uuid.uuid4().hex[m
[31m-os.environ['PIPELINE_RUN_ID'] = RUN_ID[m
[32m+[m[32mRUN_ID = os.getenv("PIPELINE_RUN_ID") or uuid.uuid4().hex[m
[32m+[m[32mos.environ["PIPELINE_RUN_ID"] = RUN_ID[m
 [m
 # Configurar logging del orquestador[m
 # Al estar en la ra√≠z, crear√° la carpeta /logs aqu√≠ mismo[m
[36m@@ -41,7 +41,7 @@[m [mclass RunIdFormatter(logging.Formatter):[m
 logging.basicConfig([m
     filename=LOG_PATH,[m
     level=logging.INFO,[m
[31m-    format="%(asctime)s - %(levelname)s - %(run_id)s - %(message)s"[m
[32m+[m[32m    format="%(asctime)s - %(levelname)s - %(run_id)s - %(message)s",[m
 )[m
 [m
 # Handler para ver la ejecuci√≥n en tiempo real por terminal[m
[36m@@ -54,9 +54,9 @@[m [mroot_logger = logging.getLogger()[m
 root_logger.addFilter(RunIdFilter(RUN_ID))[m
 for h in list(root_logger.handlers):[m
     try:[m
[32m+[m[32m        # Algunos handlers pueden no tener formatter configurado a√∫n[m
         h.setFormatter(RunIdFormatter(h.formatter._fmt))[m
     except Exception:[m
[31m-        # Si el handler no tiene un _fmt (contrario a lo esperado), configurar un formatter por defecto[m
         h.setFormatter(RunIdFormatter("%(asctime)s - %(levelname)s - %(run_id)s - %(message)s"))[m
 root_logger.addHandler(console)[m
 [m
[36m@@ -74,10 +74,24 @@[m [mdef run_pipeline():[m
         scrape_fuel_prices()[m
 [m
         logging.info("Actualizaci√≥n maestra...")[m
[31m-        update_everything()[m
[32m+[m[32m        # Importaci√≥n lazy: evita errores en pytest/IDE si el m√≥dulo no existe en tiempo de importaci√≥n[m
[32m+[m[32m        try:[m
[32m+[m[32m            from src.etl.scrapers.update_master_data import update_everything[m
[32m+[m[32m        except ImportError:[m
[32m+[m[32m            logging.getLogger(__name__).warning([m
[32m+[m[32m                "update_master_data no disponible; se omite la actualizaci√≥n maestra en esta ejecuci√≥n."[m
[32m+[m[32m            )[m
[32m+[m[32m        else:[m
[32m+[m[32m            try:[m
[32m+[m[32m                update_everything()[m
[32m+[m[32m            except Exception:[m
[32m+[m[32m                logging.exception("Fallo durante update_everything(); se contin√∫a con el pipeline.")[m
 [m
         logging.info("Enriquecimiento con clima...")[m
[31m-        get_weather_data()[m
[32m+[m[32m        try:[m
[32m+[m[32m            get_weather_data()[m
[32m+[m[32m        except Exception:[m
[32m+[m[32m            logging.exception("Fallo durante el enriquecimiento meteorol√≥gico; se contin√∫a con el pipeline.")[m
 [m
         logging.info("=== PIPELINE COMPLETADO CON √âXITO ===")[m
     except Exception as e:[m
[36m@@ -86,4 +100,4 @@[m [mdef run_pipeline():[m
 [m
 [m
 if __name__ == "__main__":[m
[31m-    run_pipeline()[m
\ No newline at end of file[m
[32m+[m[32m    run_pipeline()[m
[1mdiff --git a/src/api/shipping_etl_api.py b/src/api/shipping_etl_api.py[m
[1mindex a3df18c..9cb1c10 100644[m
[1m--- a/src/api/shipping_etl_api.py[m
[1m+++ b/src/api/shipping_etl_api.py[m
[36m@@ -1,38 +1,56 @@[m
[31m-from flask import Flask, jsonify[m
[32m+[m[32m# src/api/shipping_etl_api.py[m
[32m+[m[32mimport logging[m
 import subprocess[m
[31m-import os[m
[32m+[m[32mimport shlex[m
[32m+[m[32mfrom pathlib import Path[m
[32m+[m[32mfrom typing import Tuple[m
 [m
[31m-app = Flask(__name__)[m
[32m+[m[32mlogging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(name)s: %(message)s")[m
[32m+[m[32mlogger = logging.getLogger(__name__)[m
 [m
[31m-@app.route('/ejecutar-etl', methods=['GET'])[m
[31m-def run_etl():[m
[31m-    try:[m
[31m-        # RUTA ACTUALIZADA A SRC[m
[31m-        script_path = '/data/src/run_pipeline.py'[m
[31m-        [m
[31m-        if not os.path.exists(script_path):[m
[31m-            return jsonify({[m
[31m-                "status": "error", [m
[31m-                "message": f"Archivo no encontrado en: {script_path}"[m
[31m-            }), 404[m
[32m+[m[32mROOT = Path(__file__).resolve().parents[2][m
[32m+[m[32mETL_SCRIPT = ROOT / "src" / "etl" / "etl.py"[m
[32m+[m
[32m+[m[32mdef run_etl_process(timeout: int = 600) -> Tuple[int, str, str]:[m
[32m+[m[32m    """[m
[32m+[m[32m    Ejecuta el script ETL en un proceso separado y devuelve (returncode, stdout, stderr).[m
[32m+[m[32m    - timeout: segundos m√°ximos a esperar por la ejecuci√≥n.[m
[32m+[m[32m    """[m
[32m+[m[32m    if not ETL_SCRIPT.exists():[m
[32m+[m[32m        msg = f"Script ETL no encontrado en: {ETL_SCRIPT}"[m
[32m+[m[32m        logger.error(msg)[m
[32m+[m[32m        return 1, "", msg[m
 [m
[31m-        result = subprocess.run(['python3', script_path], capture_output=True, text=True)[m
[31m-        [m
[31m-        if result.returncode == 0:[m
[31m-            return jsonify({[m
[31m-                "status": "success", [m
[31m-                "message": "ETL de Log√≠stica completada",[m
[31m-                "output": result.stdout[m
[31m-            }), 200[m
[31m-        else:[m
[31m-            return jsonify({[m
[31m-                "status": "error", [m
[31m-                "error_python": result.stderr[m
[31m-            }), 500[m
[31m-            [m
[32m+[m[32m    cmd = f"python -u {shlex.quote(str(ETL_SCRIPT))}"[m
[32m+[m[32m    logger.info(f"Lanzando ETL: {cmd}")[m
[32m+[m[32m    try:[m
[32m+[m[32m        proc = subprocess.run([m
[32m+[m[32m            shlex.split(cmd),[m
[32m+[m[32m            capture_output=True,[m
[32m+[m[32m            text=True,[m
[32m+[m[32m            timeout=timeout[m
[32m+[m[32m        )[m
[32m+[m[32m        stdout = proc.stdout or ""[m
[32m+[m[32m        stderr = proc.stderr or ""[m
[32m+[m[32m        logger.info(f"ETL finalizado con c√≥digo {proc.returncode}")[m
[32m+[m[32m        if stdout:[m
[32m+[m[32m            logger.info(f"ETL stdout: {stdout.strip()[:1000]}")[m
[32m+[m[32m        if stderr:[m
[32m+[m[32m            logger.warning(f"ETL stderr: {stderr.strip()[:1000]}")[m
[32m+[m[32m        return proc.returncode, stdout, stderr[m
[32m+[m[32m    except subprocess.TimeoutExpired as te:[m
[32m+[m[32m        logger.exception("ETL excedi√≥ el tiempo m√°ximo permitido")[m
[32m+[m[32m        return 2, "", f"TimeoutExpired: {te}"[m
     except Exception as e:[m
[31m-        return jsonify({"status": "error", "message": str(e)}), 500[m
[32m+[m[32m        logger.exception("Error al ejecutar el ETL")[m
[32m+[m[32m        return 3, "", str(e)[m
 [m
[31m-if __name__ == '__main__':[m
[31m-    print("Servidor de ETL Log√≠stica activo en el puerto 5000...")[m
[31m-    app.run(host='0.0.0.0', port=5000)[m
\ No newline at end of file[m
[32m+[m[32mif __name__ == "__main__":[m
[32m+[m[32m    code, out, err = run_etl_process()[m
[32m+[m[32m    print(f"exit_code: {code}")[m
[32m+[m[32m    if out:[m
[32m+[m[32m        print("=== STDOUT ===")[m
[32m+[m[32m        print(out)[m
[32m+[m[32m    if err:[m
[32m+[m[32m        print("=== STDERR ===")[m
[32m+[m[32m        print(err)[m
[1mdiff --git a/src/etl/etl.py b/src/etl/etl.py[m
[1mindex 185c7ef..c50be94 100644[m
[1m--- a/src/etl/etl.py[m
[1m+++ b/src/etl/etl.py[m
[36m@@ -1,130 +1,115 @@[m
[31m-import pandas as pd[m
[31m-from pathlib import Path[m
[31m-from dotenv import load_dotenv[m
[32m+[m[32m# src/etl/etl.py[m
 import os[m
 import logging[m
[32m+[m[32mfrom pathlib import Path[m
[32m+[m[32mfrom dotenv import load_dotenv[m
[32m+[m
[32m+[m[32mimport pandas as pd[m
 from pydantic import BaseModel, Field, field_validator, ValidationError[m
[31m-from src.database import get_engine[m
 [m
[32m+[m[32mfrom src.database import get_engine[m
 [m
 load_dotenv()[m
 [m
[31m-# --- CONFIGURACI√ìN DE RUTAS ---[m
[32m+[m[32m# Rutas configurables mediante .env[m
 RAW_PATH = Path(os.getenv("RAW_DATA_PATH", "data/raw/shipping_data.csv"))[m
 CLEAN_PATH = Path(os.getenv("CLEAN_DATA_PATH", "data/clean/shipping_data_clean.csv"))[m
[31m-LOG_PATH = Path("../logs/etl.log")[m
[31m-[m
[32m+[m[32mBASE_DIR = Path(__file__).resolve().parents[3][m
[32m+[m[32mLOG_PATH = BASE_DIR / "logs" / "etl.log"[m
 LOG_PATH.parent.mkdir(parents=True, exist_ok=True)[m
 [m
[31m-# --- CONFIGURACI√ìN DE BASE DE DATOS ---[m
[31m-engine = get_engine()[m
[31m-[m
[31m-# --- LOGGING ---[m
[32m+[m[32m# Logging b√°sico[m
[32m+[m[32mlogging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(name)s: %(message)s")[m
 logger = logging.getLogger(__name__)[m
 [m
[31m-# --- ESQUEMA DE VALIDACI√ìN ---[m
[32m+[m[32m# Esquema de validaci√≥n[m
 class ShippingDataSchema(BaseModel):[m
     rank: int = Field(ge=1)[m
     state: str[m
     postal: str = Field(min_length=2, max_length=2)[m
     population: float = Field(gt=0)[m
 [m
[31m-    @field_validator('state')[m
[32m+[m[32m    @field_validator("state")[m
[32m+[m[32m    @classmethod[m
     def state_to_upper(cls, v):[m
         return v.strip().upper()[m
 [m
[31m-# --- FUNCIONES ETL ---[m
[32m+[m[32m# Funciones ETL[m
 def load_data():[m
     logger.info(f"Cargando datos desde {RAW_PATH}")[m
     if not RAW_PATH.exists():[m
[31m-        logging.error(f"No se encontr√≥ el archivo: {RAW_PATH}")[m
[32m+[m[32m        logger.error(f"No se encontr√≥ el archivo: {RAW_PATH}")[m
         raise FileNotFoundError(f"No se encontr√≥ el archivo: {RAW_PATH}")[m
[31m-    [m
     df = pd.read_csv(RAW_PATH)[m
     logger.info(f"Datos cargados correctamente. Filas: {len(df)}")[m
     return df[m
 [m
[31m-def clean_data(df):[m
[32m+[m[32mdef clean_data(df: pd.DataFrame) -> pd.DataFrame:[m
     logger.info("Iniciando limpieza y validaci√≥n de datos")[m
[31m-[m
[32m+[m[32m    # Normalizar nombres de columnas[m
[32m+[m[32m    df = df.copy()[m
     df.columns = df.columns.str.lower().str.replace(" ", "_")[m
[32m+[m[32m    # Trim strings[m
     df = df.apply(lambda col: col.str.strip() if col.dtype == "object" else col)[m
[31m-[m
[31m-    # Validaci√≥n con Pydantic[m
     valid_rows = [][m
     invalid_count = 0[m
[31m-    [m
     for index, row in df.iterrows():[m
         try:[m
[31m-            validated_row = ShippingDataSchema(**row.to_dict())[m
[31m-            valid_rows.append(validated_row.model_dump())[m
[32m+[m[32m            validated = ShippingDataSchema(**row.to_dict())[m
[32m+[m[32m            valid_rows.append(validated.model_dump())[m
         except ValidationError as e:[m
             invalid_count += 1[m
[31m-            logger.warning(f"Fila {index} descartada por error de validaci√≥n: {e.json()}")[m
[31m-[m
[31m-    if invalid_count > 0:[m
[32m+[m[32m            logger.warning(f"Fila {index} descartada por validaci√≥n: {e}")[m
[32m+[m[32m    if invalid_count:[m
         logger.info(f"Se descartaron {invalid_count} filas no v√°lidas")[m
[31m-[m
     df_clean = pd.DataFrame(valid_rows)[m
[31m-[m
[31m-    # Transformaciones adicionales[m
[32m+[m[32m    # M√©tricas adicionales[m
     if "population" in df_clean.columns and "rank" in df_clean.columns:[m
         df_clean["population_per_rank"] = df_clean["population"] / df_clean["rank"][m
         logger.info("Columna population_per_rank creada")[m
[31m-[m
     df_clean = df_clean.drop_duplicates()[m
     df_clean = df_clean.dropna(subset=["state"])[m
[31m-[m
     if "population" in df_clean.columns:[m
         df_clean = df_clean.sort_values(by="population", ascending=False)[m
[31m-[m
     logger.info(f"Limpieza completada. Filas finales: {len(df_clean)}")[m
     return df_clean[m
 [m
[31m-def save_data(df):[m
[31m-    # Guardar en CSV local[m
[32m+[m[32mdef save_data(df: pd.DataFrame):[m
     CLEAN_PATH.parent.mkdir(parents=True, exist_ok=True)[m
[31m-[m
[31m-    # A√±adir traceability id si existe[m
[31m-    run_id = os.getenv('PIPELINE_RUN_ID')[m
[32m+[m[32m    run_id = os.getenv("PIPELINE_RUN_ID")[m
     if run_id is not None:[m
         df = df.copy()[m
[31m-        df['pipeline_run_id'] = run_id[m
[31m-[m
[32m+[m[32m        df["pipeline_run_id"] = run_id[m
     df.to_csv(CLEAN_PATH, index=False)[m
     logger.info(f"Datos limpios guardados en CSV: {CLEAN_PATH}")[m
 [m
 def run_etl():[m
[31m-    logging.info("=== INICIO DEL ETL ===")[m
[32m+[m[32m    logger.info("=== INICIO DEL ETL ===")[m
     try:[m
         # 1. Extraer[m
         df = load_data()[m
[31m-        [m
[31m-        # 2. Transformar y Validar[m
[32m+[m
[32m+[m[32m        # 2. Transformar y validar[m
         df_clean = clean_data(df)[m
[31m-        [m
[31m-        # 3. Cargar local (CSV)[m
[32m+[m
[32m+[m[32m        # 3. Guardar CSV limpio[m
         save_data(df_clean)[m
[31m-        [m
[31m-        # 4. Cargar en Base de Datos (SQL)[m
[32m+[m
[32m+[m[32m        # 4. Cargar en la base de datos[m
         logger.info("Subiendo datos validados a la base de datos...")[m
[31m-        # A√±adir pipeline run id a la tabla si est√° presente[m
[31m-        run_id = os.getenv('PIPELINE_RUN_ID')[m
[31m-        if run_id is not None and 'pipeline_run_id' not in df_clean.columns:[m
[32m+[m[32m        engine = get_engine()[m
[32m+[m[32m        run_id = os.getenv("PIPELINE_RUN_ID")[m
[32m+[m[32m        if run_id is not None and "pipeline_run_id" not in df_clean.columns:[m
             df_clean = df_clean.copy()[m
[31m-            df_clean['pipeline_run_id'] = run_id[m
[31m-[m
[31m-        df_clean.to_sql([m
[31m-            name='shipping_stats', [m
[31m-            con=engine, [m
[31m-            if_exists='replace', [m
[31m-            index=False[m
[31m-        )[m
[31m-        logger.info("Tabla 'shipping_stats' actualizada con √©xito en PostgreSQL")[m
[31m-        [m
[32m+[m[32m            df_clean["pipeline_run_id"] = run_id[m
[32m+[m
[32m+[m[32m        # Es seguro usar to_sql con if_exists='replace' para pipelines idempotentes[m
[32m+[m[32m        df_clean.to_sql(name="shipping_stats", con=engine, if_exists="replace", index=False)[m
[32m+[m[32m        logger.info("Tabla 'shipping_stats' actualizada con √©xito en la base de datos")[m
         logger.info("=== ETL COMPLETADO ===")[m
     except Exception as e:[m
[31m-        logger.critical(f"Error cr√≠tico en el pipeline: {e}")[m
[32m+[m[32m        logger.critical("Error cr√≠tico en el pipeline ETL", exc_info=True)[m
[32m+[m[32m        raise[m
 [m
 if __name__ == "__main__":[m
[31m-    run_etl()[m
\ No newline at end of file[m
[32m+[m[32m    run_etl()[m
[1mdiff --git a/src/etl/scrapers/fuel_scraper.py b/src/etl/scrapers/fuel_scraper.py[m
[1mindex f62df17..8aed0c2 100644[m
[1m--- a/src/etl/scrapers/fuel_scraper.py[m
[1m+++ b/src/etl/scrapers/fuel_scraper.py[m
[36m@@ -1,4 +1,5 @@[m
 import pandas as pd[m
[32m+[m[32mfrom io import StringIO[m
 import requests[m
 from bs4 import BeautifulSoup[m
 import logging[m
[36m@@ -10,6 +11,7 @@[m [mURL = "https://gasprices.aaa.com/state-gas-price-averages/"[m
 LOG_PATH = Path(__file__).resolve().parents[3] / "logs" / "fuel_scraper.log"[m
 LOG_PATH.parent.mkdir(parents=True, exist_ok=True)[m
 [m
[32m+[m[32mlogging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(name)s: %(message)s")[m
 logger = loggin